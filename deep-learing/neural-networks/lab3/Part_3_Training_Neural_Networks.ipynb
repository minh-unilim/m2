{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gRj-x7h332N"
      },
      "source": [
        "# Lab 3 - Part 3: Training Neural Networks\n",
        "\n",
        "The network we built in the previous part isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1T-XCCztFJW4qJfRjrPTAjJvkiQ4qb_vW' width=500px> </center>\n",
        "\n",
        "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
        "\n",
        "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression  problems\n",
        "\n",
        "$$\n",
        "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
        "$$\n",
        "\n",
        "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
        "\n",
        "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1Aa-dpFavZsYUFJKAuv5xUQMvgKt0dpuQ' width=350px> </center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-bEg-Zz4Q7z"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "For single layer networks, gradient descent is straightforward to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks.\n",
        "\n",
        "Training multilayer networks is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1pc9qOQkPJmPZGsWVC-picoU2UtPxhVQD' width=550px></center>\n",
        "\n",
        "In the forward pass through the network, our data and operations go from bottom to top here. We pass the input $x$ through a linear transformation $L_1$ with weights $W_1$ and biases $b_1$. The output then goes through the sigmoid operation $S$ and another linear transformation $L_2$. Finally we calculate the loss $\\ell$. We use the loss as a measure of how bad the network's predictions are. The goal then is to adjust the weights and biases to minimize the loss.\n",
        "\n",
        "To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, we multiply the incoming gradient with the gradient for the operation. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
        "\n",
        "$$\n",
        "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
        "$$\n",
        "\n",
        "**Note:** I'm glossing over a few details here that require some knowledge of vector calculus, but they aren't necessary to understand what's going on.\n",
        "\n",
        "We update our weights using this gradient with some learning rate $\\alpha$.\n",
        "\n",
        "$$\n",
        "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
        "$$\n",
        "\n",
        "The learning rate $\\alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worDfYepJH6j"
      },
      "source": [
        "## Import resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgPMr4Wr0E4D"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFdhxHwr57Yn"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2RlQhZy0E4F"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCtUH8paXqBQ"
      },
      "outputs": [],
      "source": [
        "print('Using:')\n",
        "print('\\t\\u2022 TensorFlow version:', tf.__version__)\n",
        "print('\\t\\u2022 tf.keras version:', tf.keras.__version__)\n",
        "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zQV8MLaJOjN"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Att74swb7Ol0"
      },
      "outputs": [],
      "source": [
        "training_set, dataset_info = tfds.load('mnist', split='train', as_supervised = True, with_info = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiSe5BPrJquE"
      },
      "source": [
        "## Create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r4EMOdT9pM3"
      },
      "outputs": [],
      "source": [
        "def normalize(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255\n",
        "    return image, label\n",
        "\n",
        "num_training_examples = dataset_info.splits['train'].num_examples\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "training_batches = training_set.cache().shuffle(num_training_examples//4).batch(batch_size).map(normalize).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9SC4gnUJucy"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo2DfMVvAdbd"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
        "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "        tf.keras.layers.Dense(194, activation = 'softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TCpaAlcKCDB"
      },
      "source": [
        "## Getting the model ready for training\n",
        "\n",
        "Before we can train our model we need to set the parameters we are going to use to train it. We can configure our model for training using the `.compile` method. The main parameters we need to specify in the `.compile` method are:\n",
        "\n",
        "* **Optimizer:** The algorithm that we'll use to update the weights of our model during training. Throughout these lessons we will use the [`adam`](http://arxiv.org/abs/1412.6980) optimizer. Adam is an optimization of the stochastic gradient descent algorithm. For a full list of the optimizers available in `tf.keras` check out the [optimizers documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers#classes).\n",
        "\n",
        "\n",
        "* **Loss Function:** The loss function we are going to use during training to measure the difference between the true labels of the images in your dataset and the predictions made by your model. In this lesson we will use the `sparse_categorical_crossentropy` loss function. We use the `sparse_categorical_crossentropy` loss function when our dataset has labels that are integers, and the `categorical_crossentropy` loss function when our dataset has one-hot encoded labels. For a full list of the loss functions available in `tf.keras` check out the [losses documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses#classes).\n",
        "\n",
        "\n",
        "* **Metrics:** A list of metrics to be evaluated by the model during training. Throughout these lessons we will measure the `accuracy` of our model. The `accuracy` calculates how often our model's predictions match the true labels of the images in our dataset. For a full list of the metrics available in `tf.keras` check out the [metrics documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics#classes).\n",
        "\n",
        "These are the main parameters we are going to set throught these lesson. You can check out all the other configuration parameters in the [TensorFlow documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYv3pv5-InR1"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5CjYa8ES3OI"
      },
      "source": [
        "## Taking a look at the loss and accuracy before training\n",
        "\n",
        "Before we train our model, let's take a look at how our model performs when it is just using random weights. Let's take a look at the `loss` and `accuracy` values when we pass a single batch of images to our un-trained model. To do this, we will use the `.evaluate(data, true_labels)` method. The `.evaluate(data, true_labels)` method compares the predicted output of our model on the given `data` with the given `true_labels` and returns the `loss` and `accuracy` values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_7aijzvJQZ7"
      },
      "outputs": [],
      "source": [
        "for image_batch, label_batch in training_batches.take(1):\n",
        "    loss, accuracy = model.evaluate(image_batch, label_batch)\n",
        "\n",
        "print('\\nLoss before training: {:,.3f}'.format(loss))\n",
        "print('Accuracy before training: {:.3%}'.format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvsfbLEMZjZ5"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Now let's train our model by using all the images in our training set. Some nomenclature, one pass through the entire dataset is called an *epoch*. To train our model for a given number of epochs we use the `.fit` method, as seen below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-CgmnKBZDjq"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(training_batches, epochs = EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFgG_WfUjCic"
      },
      "source": [
        "The `.fit` method returns a `History` object which contains a record of training accuracy and loss values at successive epochs, as well as validation accuracy and loss values when applicable. We will discuss the history object in a later lesson.\n",
        "\n",
        "With our model trained, we can check out it's predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghr7z-SnctRw"
      },
      "outputs": [],
      "source": [
        "for image_batch, label_batch in training_batches.take(1):\n",
        "    ps = model.predict(image_batch)\n",
        "    first_image = image_batch.numpy().squeeze()[0]\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
        "ax1.axis('off')\n",
        "ax2.barh(np.arange(10), ps[0])\n",
        "ax2.set_aspect(0.1)\n",
        "ax2.set_yticks(np.arange(10))\n",
        "ax2.set_yticklabels(np.arange(10))\n",
        "ax2.set_title('Class Probability')\n",
        "ax2.set_xlim(0, 1.1)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4WcPdR9jKMB"
      },
      "source": [
        "WOW!! Now our network is brilliant. It can accurately predict the digits in our images. Let's take a look again at the loss and accuracy values for a single batch of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFZKBfTgfPVy"
      },
      "outputs": [],
      "source": [
        "for image_batch, label_batch in training_batches.take(1):\n",
        "    loss, accuracy = model.evaluate(image_batch, label_batch)\n",
        "\n",
        "\n",
        "print('\\nLoss after training: {:,.3f}'.format(loss))\n",
        "print('Accuracy after training: {:.3%}'.format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa5_vwtotNeg"
      },
      "source": [
        "> **Exercise:** Create a network with 784 input units, a hidden layer with 128 units, then a hidden layer with 64 units, then a hidden layer with 32 units and finally an output layer with 10 units. Use a ReLu activation function for all the hidden layers and a softmax activation function for the output layer. Then compile the model using an `adam` optimizer, a `sparse_categorical_crossentropy` loss function, and the `accuracy` metric. Finally, print the loss and accuracy of your un-trained model for a single batch of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txuSaeuirvgc"
      },
      "outputs": [],
      "source": [
        "## your Solution\n",
        "Model =\n",
        "\n",
        "\n",
        "# optimizer\n",
        "Model.compile(optimizer=,\n",
        "             loss=,\n",
        "             metrics=\n",
        "             )\n",
        "\n",
        "\n",
        "for image_batch, label_batch in training_batches.take(1):\n",
        "    loss, accuracy = Model.evaluate(image_batch, label_batch)\n",
        "\n",
        "\n",
        "\n",
        "print('\\nLoss before training: {:,.3f}'.format(loss))\n",
        "print('Accuracy before training: {:.3%}'.format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgdaQEVUumxo"
      },
      "source": [
        "> **Exercise:** Train the model you created above for 5 epochs and then print the loss and accuracy of your trained model for a single batch of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzxZtgBDt3Ak"
      },
      "outputs": [],
      "source": [
        "## Solution\n",
        "history=\n",
        "\n",
        "\n",
        "print('\\nLoss after training: {:,.3f}'.format(loss))\n",
        "print('Accuracy after training: {:.3%}'.format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfBqrMikvVCY"
      },
      "source": [
        "> **Exercise:** Plot the prediction of the model you created and trained above on a single image from the training set. Also plot the probability predicted by your model for each digit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOwMUqYzvKtK"
      },
      "outputs": [],
      "source": [
        "## Solution\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqREWxKKVwql"
      },
      "source": [
        "## Automatic Differentiation\n",
        "\n",
        "Let's now take a minute to see how TensorFlow calculates and keeps track of the gradients needed for backpropagation. TensorFlow provides a class that records automatic differentiation operations, called `tf.GradientTape`. Automatic differentiation, also known as algorithmic differentiation or simply “autodiff”, is a family of techniques used by computers for efficiently and accurately evaluating derivatives of numeric functions.\n",
        "\n",
        "`tf.GradientTape` works by keeping track of operations performed on tensors that are being \"watched\". By default `tf.GradientTape` will automatically \"watch\" any trainable variables, such as the weights in our model. Trainable variables are those that have `trainable=True`. When we create a model with `tf.keras`, all of the parameters are initialized with `trainable = True`. Any tensor can also be manually \"watched\" by invoking the watch method.\n",
        "\n",
        "\n",
        "Let's see a simple example. Let's take the following equation:\n",
        "\n",
        "$$\n",
        "y = x^2\n",
        "$$\n",
        "\n",
        "The derivative of `y` with respect to `x` is given by:\n",
        "\n",
        "$$\n",
        "\\frac{d y}{d x} = 2x\n",
        "$$\n",
        "\n",
        "Now, let's use `tf.GradientTape` to calculate the derivative of a tensor `y` with respect to a tensor `x`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-ktpx5dVU3O"
      },
      "outputs": [],
      "source": [
        "# Set the random seed so things are reproducible\n",
        "tf.random.set_seed(7)\n",
        "\n",
        "# Create a random tensor\n",
        "x = tf.random.normal((2,2))\n",
        "\n",
        "# Calculate gradient\n",
        "with tf.GradientTape() as g:\n",
        "    g.watch(x)\n",
        "    y = x ** 2\n",
        "\n",
        "dy_dx = g.gradient(y, x)\n",
        "\n",
        "# Calculate the actual gradient of y = x^2\n",
        "true_grad = 2 * x\n",
        "\n",
        "# Print the gradient calculated by tf.GradientTape\n",
        "print('Gradient calculated by tf.GradientTape:\\n', dy_dx)\n",
        "\n",
        "# Print the actual gradient of y = x^2\n",
        "print('\\nTrue Gradient:\\n', true_grad)\n",
        "\n",
        "# Print the maximum difference between true and calculated gradient\n",
        "print('\\nMaximum Difference:', np.abs(true_grad - dy_dx).max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgLCJaooV5Un"
      },
      "source": [
        "The `tf.GradientTape` class keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor.\n",
        "\n",
        "To know more about `tf.GradientTape` and trainable variables check the following links\n",
        "\n",
        "* [Gradient Tape](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape)\n",
        "\n",
        "* [TensorFlow Variables](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Variable)\n",
        "\n",
        "Next up you'll write the code for training a neural network on a more complex dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am0SvU9KWAD3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}