{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/labai.png\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with GRU\n",
    "\n",
    "In this exercise your goal is to build text generation model with GRU model by complete all piece of code below, you can add or change code as we can\n",
    "\n",
    "\n",
    "**Objective**:  \n",
    "In this exercise, your goal is to build a text generation model using a Gated Recurrent Unit (GRU). You will complete all the provided code segments and are encouraged to add or modify code to improve the model. The key steps involve:\n",
    "\n",
    "1. Preprocessing the text data.\n",
    "2. Implementing the GRU-based neural network.\n",
    "3. Training the model on the provided dataset.\n",
    "4. Generating new text based on a seed sequence.\n",
    "\n",
    "**Instructions**:\n",
    "- Follow the code structure provided and complete the missing sections.\n",
    "- Experiment with different hyperparameters to improve performance.\n",
    "- You are free to adjust the code as needed to enhance results.\n",
    "\n",
    "**Please use Google colab for free GPU**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# import sommes packages\n",
    "import re\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import unicodedata\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab  import build_vocab_from_iterator\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Attempt GPU; if not, stay on CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I- Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "text = Path('./data/tiny-shakespeare.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file: 1,115,394\n"
     ]
    }
   ],
   "source": [
    "# print total number of characters:\n",
    "print(f'Number of characters in text file: {len(text):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "IwwOe-tJ-xcE",
    "outputId": "00300e0a-c41d-457d-e88a-e8a13004670b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(text[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Word-Based Text Generation\n",
    "\n",
    "The first model you'll build for **text generation** will use Word-based tokens. Each token will be a single word from the text and the model will learn to predict the next word (a token).\n",
    "\n",
    "To generate text, the model will take in a new string, word-by-word, and then generate a new likely word based on the past input. Then the model will take into account that new word and generate the following word and so on and so on until the model has produced a set number of word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1  Tokenization : \n",
    "Create a tokenizer that will create tokens by character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class  WordTokenizer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab: torchtext.vocab.Vocab|Dict[str,int])-> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(vocab, torchtext.vocab.Vocab):\n",
    "            self.token2id=vocab.get_stoi()\n",
    "            self.id2token={id:ch for ch,id in vocab.get_stoi().items()}\n",
    "            self.vocab_size=len(self.token2id)\n",
    "            \n",
    "        elif isinstance(vocab, dict):\n",
    "            self.token2id=vocab\n",
    "            self.id2token={id:ch for ch,id in vocab.items()}\n",
    "            self.vocab_size=len(self.token2id)\n",
    "            \n",
    "        else:\n",
    "            raise TypeError(\"Please loads a vocabulary file into a dictionary \\\n",
    "                            Dict[str,int] or torchtext.vocab.Vocab\")\n",
    "    \n",
    "    def encode(self, text:List[str]|str):\n",
    "        if isinstance(text, str):\n",
    "            text_list=self.tokenize(text)\n",
    "            \n",
    "        tokenid=[]\n",
    "        for token in text_list:\n",
    "            tokenid.append(self.token2id[token])\n",
    "        return  torch.tensor(tokenid,  dtype=torch.long)\n",
    "\n",
    "    \n",
    "    def decode(self, idx:torch.tensor):\n",
    "        #idx: torch.Tensor containing integers\n",
    "        token=[]\n",
    "        for id in idx.tolist():\n",
    "            token.append(self.id2token[id])\n",
    "        return ' '.join(token)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> List[str]:\n",
    "        # Normalize text by lowercasing and removing extra spaces\n",
    "        text = text.lower().strip()\n",
    "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "        \n",
    "        return tokens\n",
    "        \n",
    "    @staticmethod \n",
    "    def _tokenizer_corpus(corpus:List[str]):\n",
    "        for text in corpus:\n",
    "            yield WordTokenizer.tokenize(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_from_text(text: str) -> List[str]:\n",
    "        \"\"\"build vocab from one text corpus\"\"\"\n",
    "        vocab=build_vocab_from_iterator(WordTokenizer._tokenizer_corpus(WordTokenizer.tokenize(text)),\n",
    "                                        specials=[\"<unk>\"]\n",
    "                                       )\n",
    "        vocab.set_default_index(vocab[\"<unk>\"])\n",
    "        \n",
    "        return WordTokenizer(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer from text\n",
    "tokenizer = WordTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'citizen', ':', 'before', 'we', 'proceed', 'any', 'further', ',', 'hear', 'me', 'speak', '.', 'all', ':', 'speak', ',', 'speak', '.', 'first', 'citizen', ':', 'you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish', '?', 'all', ':', 'resolved', '.', 'resolved', '.', 'first', 'citizen', ':', 'first', ',', 'you', 'know', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people', '.', 'all', ':', 'we', 'know', \"'\", 't', ',', 'we', 'know', \"'\", 't', '.', 'first', 'citizen', ':', 'let', 'us']\n"
     ]
    }
   ],
   "source": [
    "# show example of word-based tokens\n",
    "print(tokenizer.tokenize(text[0:300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 312,    8,    4,  561, 3008,  667,    3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "encode_text=tokenizer.encode(\"Welcome to the deep learning course.\")\n",
    "encode_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'welcome to the deep learning course .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_text=tokenizer.decode(encode_text)\n",
    "decode_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - Prepare dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class shakespeareDataset(Dataset):\n",
    "    def __init__(self, encode_text, max_seq_length: int):\n",
    "        self.encode_text     = encode_text\n",
    "        self.max_seq_length  = max_seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encode_text)-self.max_seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self.encode_text)-self.max_seq_length\n",
    "        \n",
    "        x_train= self.encode_text[idx:idx+self.max_seq_length]\n",
    "        \n",
    "        # Target is shifted by one character/token\n",
    "        y_target= self.encode_text[idx+1:idx+1+self.max_seq_length]\n",
    "        \n",
    "        return x_train, y_target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=shakespeareDataset(encode_text=tokenizer.encode(text),max_seq_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"first citizen : before we proceed any further , hear me speak . all : speak , speak . first citizen : you are all resolved rather to die than to famish ? all : resolved . resolved . first citizen : first , you know caius marcius is chief enemy to the people . all : we know ' t , we know ' t . first citizen : let us kill him , and we ' ll have corn at our own price . is ' t a verdict ? all : no more talking on ' t\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "tokenizer.decode(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"citizen : before we proceed any further , hear me speak . all : speak , speak . first citizen : you are all resolved rather to die than to famish ? all : resolved . resolved . first citizen : first , you know caius marcius is chief enemy to the people . all : we know ' t , we know ' t . first citizen : let us kill him , and we ' ll have corn at our own price . is ' t a verdict ? all : no more talking on ' t ;\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "tokenizer.decode(dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch dataset\n",
    "train_dataloader = DataLoader(dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11467"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build GRU model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUTextGen(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, \n",
    "                 num_layers: int, dropout: float = 0.5):\n",
    "        \n",
    "        super(GRUTextGen, self).__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        assert 0 <= self.dropout <= 1, \"dropout value must be between [0,1]\"\n",
    "        \n",
    "        # Embedding layer to convert token indices to embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                      embedding_dim=embedding_dim)\n",
    "        \n",
    "        # GRU layer to process the sequence. The input size is the embedding dimension.\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, \n",
    "                          hidden_size=hidden_dim, \n",
    "                          num_layers=num_layers, \n",
    "                          dropout=self.dropout if num_layers > 1 else 0, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to map GRU output to vocabulary size\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x: torch.tensor):\n",
    "        # x: Tensor with shape (batch_size, sequence_length)\n",
    "        assert x.ndim == 2, \"x tensor must be 2D dimensions with shape (B,S), B=batch, S=sequence length\"\n",
    "        \n",
    "        # Pass input through embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        # Pass through GRU\n",
    "        output, h = self.gru(x)  # output: (batch_size, sequence_length, hidden_dim)\n",
    "        # print(output.shape)\n",
    "        \n",
    "        # Pass the GRU output through the fully connected layer to generate logits\n",
    "        logits = self.fc(output)  # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # print(logits.shape)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 11467])\n"
     ]
    }
   ],
   "source": [
    "# Example parameters\n",
    "vocab_size = 11467\n",
    "embedding_dim = 128\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "# Instantiate model\n",
    "GRU_model = GRUTextGen(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "\n",
    "# Example input (batch_size=32, sequence_length=20)\n",
    "x = torch.randint(0, vocab_size, (32, 20))\n",
    "\n",
    "# Forward pass\n",
    "logits = GRU_model(x)\n",
    "\n",
    "print(logits.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference mode: Define Text Generation :\n",
    "Generate text with a character-based model\n",
    "\n",
    "The `generate_text_by_word` function will use your tokenizer and LSTM model to generate new text token-by-token by taking in the input text and token sampling parameters. We can use temperature and top-k sampling to adjust the \"creativeness\" of the generated text.\n",
    "\n",
    "We also pass in the num_tokens parameter to tell the function how many tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text_by_word(input_text:str, max_tokens:int=15, \n",
    "                          temperature:int=1, top_k:int|None=None, \n",
    "                          do_sample:bool=False, \n",
    "                        tokenizer=tokenizer):\n",
    "    \n",
    "    \"\"\"Inference: Define Text Generation\"\"\"\n",
    "    idx=tokenizer.encode(input_text).unsqueeze(dim=0)\n",
    "\n",
    "    max_sequence_length=31\n",
    "        \n",
    "    assert idx.ndim==2, \"input token must be 2D with sahpe (B, S) B batch,S sequence Length\"\n",
    "        \n",
    "    for _ in range(max_tokens): # The maximum number of tokens that can be generated\n",
    "        # if the sequence context is growing too long we must crop it at context_size\n",
    "        idx_cond=idx if idx.size(1)<=max_sequence_length else idx[:,-max_sequence_length:]\n",
    "        \n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits=GRU_model(idx_cond)\n",
    "        \n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        if top_k is not None:\n",
    "            values= torch.topk(logits, top_k).values\n",
    "            logits[logits < values[:,[-1]]]=-torch.inf \n",
    "                \n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs =F.softmax(logits, dim=-1)\n",
    "\n",
    "        if do_sample:\n",
    "            idx_next=torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next=torch.topk(probs, k=1, dim=-1).indices  # greedy decoding\n",
    "               \n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "    return tokenizer.decode(idx.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to be or not to be kerns apoplexy meddler vassal touch mars martino awaking horror unconstant track positively horror doves springing'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check text generation without training model\n",
    "TEST_PHRASE = 'To be or not to be'\n",
    "generate_text_by_word(TEST_PHRASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GRU : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n",
      "torch.Size([64, 100]) torch.Size([64, 100, 11467])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [64, 11467], got [64, 100]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m output \u001b[38;5;241m=\u001b[39m GRU_model(X_batch)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_batch\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 21\u001b[0m loss   \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Projects/m2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/m2/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/m2/.venv/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [64, 11467], got [64, 100]"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(GRU_model.parameters(), lr=0.01)\n",
    "\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    GRU_model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(X_batch.shape)\n",
    "        \n",
    "        output = GRU_model(X_batch)\n",
    "\n",
    "        print(y_batch.shape, output.shape)\n",
    "\n",
    "        loss   = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_dataloader)}')\n",
    "    print('-'*72)\n",
    "    \n",
    "    gen_output = generate_text_by_word(\n",
    "        input_text=TEST_PHRASE,\n",
    "        temperature=0.8,\n",
    "        max_tokens=30,\n",
    "        top_k=None, \n",
    "        do_sample=False, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens  to generate and observe how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_text_by_char(\n",
    "    input_text='To be or ',\n",
    "    max_tokens=20,\n",
    "    do_sample=False, \n",
    "    tokenizer=tokenizer,\n",
    "    temperature=1.0,\n",
    "    topk=None,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great Job üëè "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
