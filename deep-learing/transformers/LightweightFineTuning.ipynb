{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "Our goal is to compare traditional Full fine-tuning BERT model on sentiment analysis task (actually DistilBERT) with PEFT Bert model on the same tasks using the Hugging Face Transformers library. For evaluation we use accurary approach and We will use the IMDB movie review dataset to train and evaluate Bert model. The IMDB dataset contains movie reviews that are labeled as either positive or negative.\n",
    "\n",
    "* PEFT technique : LoRA\n",
    "* Model : BERT \n",
    "* Evaluation approach : Accuracy\n",
    "* Fine-tuning dataset :  IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ecd0f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory, accelerate-launch and accelerate-merge-weights are installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script datasets-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install and update some packages first\n",
    "\n",
    "### then restart the kernel to use updated packages ###\n",
    "\n",
    "!pip install -q --upgrade datasets transformers[torch] peft "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "In the cells below, We load Bert pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory is : /workspace\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Import the datasets packages from HugginFace\n",
    "from datasets import load_dataset_builder, load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          DataCollatorWithPadding, Trainer, TrainingArguments)\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"current directory is :\", os.getcwd())\n",
    "\n",
    "# Attempt GPU; if not, stay on CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60f02a7f99949d8b17762ace1acf749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7036e33f84504e75b34d7a741963c29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42b5990d6a6429f920b9994b4e44cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33ce87d6f6148b283b809d40603c322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84812a648ed4f0d89214e5127451905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60577e9c6214b489b18c6f1ff2072ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc18978359c4728b9196d084cd4e7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the train and test splits of the imdb dataset\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "dataset={split:ds for split, ds in zip(splits, load_dataset(\"imdb\",split=splits))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5adfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " })}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We choose smaller subset of dataset to fine-tune to reduce the time it takes to train\n",
    "ds={split:dataset[split].shuffle(seed=42).select(range(500)) for split in splits}\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c702eb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...',\n",
       " 'This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra\\'s song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.',\n",
       " 'George P. Cosmatos\\' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn\\'t win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn\\'t appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show few lines of text\n",
    "ds['train'][\"text\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label={0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "# show few lines of label (1=pos,0=neg)\n",
    "ds['train'][\"label\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e77deb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br />Keep up the good work guys, you CAN and DO make a difference.<br /><br />\",\n",
       " \"This is the latest entry in the long series of films with the French agent, O.S.S. 117 (the French answer to James Bond). The series was launched in the early 1950's, and spawned at least eight films (none of which was ever released in the U.S.). 'O.S.S.117:Cairo,Nest Of Spies' is a breezy little comedy that should not...repeat NOT, be taken too seriously. Our protagonist finds himself in the middle of a spy chase in Egypt (with Morroco doing stand in for Egypt) to find out about a long lost friend. What follows is the standard James Bond/Inspector Cloussou kind of antics. Although our man is something of an overt xenophobe,sexist,homophobe, it's treated as pure farce (as I said, don't take it too seriously). Although there is a bit of rough language & cartoon violence, it's basically okay for older kids (ages 12 & up). As previously stated in the subject line, just sit back,pass the popcorn & just enjoy.\",\n",
       " 'This movie was so frustrating. Everything seemed energetic and I was totally prepared to have a good time. I at least thought I\\'d be able to stand it. But, I was wrong. First, the weird looping? It was like watching \"America\\'s Funniest Home Videos\". The damn parents. I hated them so much. The stereo-typical Latino family? I need to speak with the person responsible for this. We need to have a talk. That little girl who was always hanging on someone? I just hated her and had to mention it. Now, the final scene transcends, I must say. It\\'s so gloriously bad and full of badness that it is a movie of its own. What crappy dancing. Horrible and beautiful at once.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show few lines of text\n",
    "ds['test'][\"text\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a153ffac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show few lines of label (1=pos,0=neg)\n",
    "ds['test'][\"label\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a714a53",
   "metadata": {},
   "source": [
    "###  Tokenize text\n",
    "\n",
    "Models cannot process raw text, so youâ€™ll need to convert the text into numbers (integers). Tokenization provides a way to do this by dividing text into individual words called tokens. Tokens are finally converted to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0052f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e4f369762d492eaca59d128bc8b275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b9e897981e4cefb0eeb048f1445396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434e1636ea8e4ff4b050f052d9eb4c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1b82d486ac4305bbbd2dd343de15fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer for Bert\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    \"\"\"\n",
    "    tokenize your entire dataset\n",
    "\n",
    "    \"\"\"\n",
    "    return tokenizer(example[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd481165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a284c94ab3394a6d973971b9f94e5177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94507506d4a743809b03360fe448dbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_ds={split:ds[split].map(tokenization, batched=True) for split in splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c926010a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "tokenize_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1b564",
   "metadata": {},
   "source": [
    "### Train Bert model\n",
    "\n",
    "Now it's time to train our model. We'll use the Trainer class from the ðŸ¤— Transformers library to do this. The Trainer class provides a high-level API that abstracts away a lot of the training loop.\n",
    "\n",
    "First we'll define a function to compute our accuracy metreic then we make the Trainer.\n",
    "\n",
    "Let's take this opportunity to learn about the DataCollator. According to the HuggingFace documentation:\n",
    "\n",
    "Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.\n",
    "\n",
    "To be able to build batches, data collators may apply some processing (like padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "160dfb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model=AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=2,\n",
    "        id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},  # For converting predictions to strings\n",
    "        label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "       \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a81e84cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./result_non_PEFT/sentiment_analysis\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,   \n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.001,  \n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",         \n",
    "    save_strategy=\"epoch\",         \n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f50af8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5028fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenize_ds[\"train\"],\n",
    "    eval_dataset=tokenize_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer, padding=True, return_tensors=\"pt\"),\n",
    "    compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5ca62ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.646400</td>\n",
       "      <td>0.522028</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.291600</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.463799</td>\n",
       "      <td>0.852000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=0.3514350006977717, metrics={'train_runtime': 113.9992, 'train_samples_per_second': 13.158, 'train_steps_per_second': 0.842, 'total_flos': 198701097984000.0, 'train_loss': 0.3514350006977717, 'epoch': 3.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run trainer to train bert model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd9a20",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Evaluating the model is as simple as calling the evaluate method on the trainer object. This will run the model on the test set and compute the metrics we specified in the compute_metrics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7cc515",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "df = pd.DataFrame(tokenize_ds[\"test\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Replace <br /> tags in the text with spaces\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"<br />\", \" \")\n",
    "\n",
    "# display few row of raw data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the model predictions to the dataframe\n",
    "predictions = trainer.predict(tokenize_ds[\"test\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "In the cells below, We create a PEFT model from the loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "Lora_config = LoraConfig(task_type=TaskType.SEQ_CLS,\n",
    "                         target_modules=[\"q_lin\",\"k_lin\",\"v_lin\"],\n",
    "                         inference_mode=False, \n",
    "                         r=8, lora_alpha=32, \n",
    "                         lora_dropout=0.1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PeftModel with the get_peft_model() function It takes a base model, which you can load from the Transformers library \n",
    "# and the LoraConfig containing the parameters for how to configure a model for training with LoRA.\n",
    "\n",
    "peft_bert_model=get_peft_model(bert_model,Lora_config)\n",
    "\n",
    "peft_bert_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465630fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_bert_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bbdc34",
   "metadata": {},
   "source": [
    "### Train peft model\n",
    "\n",
    "Each PEFT method is defined by a PeftConfig class that stores all the important parameters for building a PeftModel. For example, to train with LoRA, load and create a LoraConfig class and specify the following parameters:\n",
    "\n",
    "- task_type: the task to train for (text classification language modeling in this case)\n",
    "- inference_mode: whether youâ€™re using the model for inference or not\n",
    "- r: the dimension of the low-rank matrices\n",
    "- lora_alpha: the scaling factor for the low-rank matrices\n",
    "- lora_dropout: the dropout probability of the LoRA layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_peft=TrainingArguments(\n",
    "    output_dir=\"./result_PEFT/sentiment_analysis\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,   \n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",         \n",
    "    save_strategy=\"epoch\",         \n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer = Trainer(\n",
    "    model=peft_bert_model,\n",
    "    args=training_args_peft,\n",
    "    train_dataset=tokenize_ds[\"train\"],\n",
    "    eval_dataset=tokenize_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer, padding=True, return_tensors=\"pt\"),\n",
    "    compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run trainer to train bert model\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the peft model\n",
    "peft_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "dfs = pd.DataFrame(tokenize_ds[\"test\"])\n",
    "dfs = dfs[[\"text\", \"label\"]]\n",
    "\n",
    "# Replace <br /> tags in the text with spaces\n",
    "dfs[\"text\"] = dfs[\"text\"].str.replace(\"<br />\", \" \")\n",
    "\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the model predictions to the dataframe\n",
    "predictions = peft_trainer.predict(tokenize_ds[\"test\"])\n",
    "dfs[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the PEFT model.\n",
    "peft_bert_model.save_pretrained(\"lora_bert_model\")\n",
    "\n",
    "# save bert model\n",
    "bert_model.save_pretrained(\"Bert_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoPeftModelForSequenceClassification.from_pretrained(\"lora_bert_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "input_text = \"I love Udacity deep learning course\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "predictions= model(**inputs).logits.cpu().detach().numpy()\n",
    "\n",
    "print('The sentiment anlysis of this `{input_text}` is:',label[np.argmax(predictions[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ca981",
   "metadata": {},
   "outputs": [],
   "source": [
    "BertModel = AutoModelForSequenceClassification.from_pretrained(\"Bert_Model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "Model = BertModel.to(device)\n",
    "Model.eval()\n",
    "\n",
    "input_text = \"I love Udacity deep learning course\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "predictions= Model(**inputs).logits.cpu().detach().numpy()\n",
    "\n",
    "print(f'The sentiment anlysis of this `{input_text}` is:',label[np.argmax(predictions[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2037047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a5da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
