{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Literal, List, Callable, Any, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, name, next_state, reward, prob = 1):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"\n",
    "    Markov Decision Process\n",
    "        horizon: number of time steps, excluding start and terminal\n",
    "        state_space: all possible state names\n",
    "        action_space: all possible action names\n",
    "        actions: (time, state) => {action: (prob, next_state, reward)[]}, there must be one possible action at the horizon\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 horizon: Union[int, Literal['infty']], \n",
    "                 state_space: List[str],\n",
    "                 action_space: List[str],\n",
    "                 actions # (time, state) => {action : (prob, next_state, reward)}[]\n",
    "                 ):\n",
    "        self.horizon = horizon\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.actions = actions\n",
    "\n",
    "    def get_actions(self, time: int, state: str):\n",
    "        \"\"\"Returns available actions for a given state and time.\"\"\"\n",
    "        return self.actions(time, state)\n",
    "    \n",
    "    def get_response(self, time: int, state: str, action: str):\n",
    "        actions = self.actions(time, state)\n",
    "        if action not in actions.keys():\n",
    "            raise Exception(f\"Valid actions are {actions.keys()}\")\n",
    "        return actions[action]\n",
    "    \n",
    "    def get_expected_reward(self, time: int, state: str, action: str):\n",
    "        response = self.get_response(time, state, action)\n",
    "        return np.sum([next_state[0]*next_state[2] for next_state in response])\n",
    "    \n",
    "    def evaluate_policy(self, policy: Callable[[int, str], Tuple[float, str]], \n",
    "                        time, state, \n",
    "                        method: Union[Literal['dp'], Literal['iter']]):\n",
    "        \n",
    "        # If at the horizon, return the reward\n",
    "        if time == self.horizon:\n",
    "            return list(self.actions(self.horizon, state).values())[0][0][2]\n",
    "        \n",
    "        current_policy = policy(time, state)\n",
    "\n",
    "        eval = 0\n",
    "        for action, prob in current_policy.items():\n",
    "            response = self.get_response(time, state, action)\n",
    "            eval += prob*np.sum([next_state[0]*(next_state[2]+self.evaluate_policy(policy, time+1, next_state[1])) for next_state in response])\n",
    "\n",
    "        return eval\n",
    "    \n",
    "    def summary(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.8\n",
    "N = 3\n",
    "\n",
    "''' Returns possible action {action: (prob, next state, reward)} '''\n",
    "def actions(time, state):\n",
    "    if time < N:\n",
    "        if state == 'free':\n",
    "            return {\n",
    "                'move': [(p, 'free', 0), (1-p, 'parked', 0)],\n",
    "                'park': [(1, 'terminal', time)]\n",
    "            }\n",
    "        elif state == 'parked':\n",
    "            return {\n",
    "                'move': [(p, 'free', 0), (1-p, 'parked', 0)]\n",
    "            }\n",
    "    else:\n",
    "        if state == 'free':\n",
    "            return {\n",
    "                'park': [(1, 'terminal', N)]\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'move': [(1, 'terminal', 0)]\n",
    "            }\n",
    "\n",
    "mdp = MDP(N, ['free', 'parked', 'terminal'], ['move', 'park'], actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(time: int, state: str):\n",
    "    if state == 'free':\n",
    "        return {'park': 0.5, 'move': 0.5}\n",
    "    return {'move': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'move': [(0.8, 'free', 0), (0.19999999999999996, 'parked', 0)]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.actions(1, \"parked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.evaluate_policy(policy, 1, \"parked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.get_expected_reward(1, \"free\", \"move\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy Evaluation\n",
      "Iteration 1: Policy Improvement\n",
      "Policy converged after 1 iterations.\n",
      "Optimal Policy:\n",
      "State free: Action move\n",
      "State parked: Action move\n",
      "\n",
      "Optimal Value Function:\n",
      "State free: Value 0.0\n",
      "State parked: Value 0.0\n",
      "State terminal: Value 0.0\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "class PolicyIteration:\n",
    "    def __init__(self, mdp: MDP, gamma: float = 1.0):\n",
    "        self.mdp = mdp\n",
    "        self.gamma = gamma\n",
    "        self.value_function = {state: 0.0 for state in self.mdp.state_space}\n",
    "        self.policy = {state: np.random.choice(self.mdp.action_space) for state in self.mdp.state_space if state != 'terminal'}\n",
    "\n",
    "    def evaluate_policy(self, tolerance: float = 1e-6):\n",
    "        \"\"\"Evaluate the current policy until convergence.\"\"\"\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in self.mdp.state_space:\n",
    "                if state == 'terminal':\n",
    "                    continue\n",
    "                v = self.value_function[state]\n",
    "                action = self.policy[state]\n",
    "                transitions = self.mdp.get_transitions(0, state, action)\n",
    "                new_value = sum(prob * (reward + self.gamma * self.value_function[next_state])\n",
    "                                for prob, next_state, reward in transitions)\n",
    "                self.value_function[state] = new_value\n",
    "                delta = max(delta, abs(v - new_value))\n",
    "            if delta < tolerance:\n",
    "                break\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        \"\"\"Improve the current policy based on the value function.\"\"\"\n",
    "        policy_stable = True\n",
    "        for state in self.mdp.state_space:\n",
    "            if state == 'terminal':\n",
    "                continue\n",
    "            old_action = self.policy[state]\n",
    "            action_values: Dict[str, float] = {}\n",
    "            for action in self.mdp.get_actions(state, 0):\n",
    "                transitions = self.mdp.get_transitions(0, state, action)\n",
    "                action_values[action] = sum(prob * (reward + self.gamma * self.value_function[next_state])\n",
    "                                            for prob, next_state, reward in transitions)\n",
    "            best_action = max(action_values, key=action_values.get)\n",
    "            self.policy[state] = best_action\n",
    "            if best_action != old_action:\n",
    "                policy_stable = False\n",
    "        return policy_stable\n",
    "\n",
    "    def iterate_policy(self):\n",
    "        \"\"\"Run policy iteration until the policy is stable.\"\"\"\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            print(f\"Iteration {iteration}: Policy Evaluation\")\n",
    "            self.evaluate_policy()\n",
    "            print(f\"Iteration {iteration}: Policy Improvement\")\n",
    "            if self.policy_improvement():\n",
    "                print(f\"Policy converged after {iteration} iterations.\")\n",
    "                break\n",
    "        return self.policy, self.value_function\n",
    "\n",
    "\n",
    "# Example usage\n",
    "mdp = MDP(\n",
    "    horizon=N,\n",
    "    state_space=['free', 'parked', 'terminal'],\n",
    "    action_space=['move', 'park'],\n",
    "    transition=transition\n",
    ")\n",
    "\n",
    "pi = PolicyIteration(mdp)\n",
    "optimal_policy, optimal_value_function = pi.iterate_policy()\n",
    "\n",
    "print(\"Optimal Policy:\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"State {state}: Action {action}\")\n",
    "\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "for state, value in optimal_value_function.items():\n",
    "    print(f\"State {state}: Value {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
